"""
MongoDB ê¸°ë°˜ ë²¡í„° ìŠ¤í† ì–´ v2.0
CREATED [2024-12-19]: MongoDB text_embedding í•„ë“œë¥¼ í™œìš©í•œ ë²¡í„° ê²€ìƒ‰

ê¸°ëŠ¥:
- MongoDB ë‚´ì¥ ë²¡í„° ê²€ìƒ‰
- FAISS/Chroma ë²¡í„° DBì—ì„œ MongoDBë¡œ ì„ë² ë”© ì´ê´€
- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì§€ì›
"""

from langchain.schema import Document
from typing import List, Dict, Any, Optional, Union
import numpy as np
from bson import ObjectId
import logging
from src.utils.database_v2 import MongoDBClientV2

logger = logging.getLogger(__name__)

class MongoVectorStore:
    """MongoDB ê¸°ë°˜ ë²¡í„° ìŠ¤í† ì–´"""
    
    def __init__(self, mongodb_client: MongoDBClientV2):
        self.db_client = mongodb_client
        self.collection = mongodb_client.documents
    
    def add_documents(self, documents: List[Document], embeddings: List[List[float]], 
                     folder_id: ObjectId) -> List[ObjectId]:
        """ë¬¸ì„œì™€ ì„ë² ë”©ì„ MongoDBì— ì €ì¥"""
        document_ids = []
        
        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):
            document_id = self.db_client.insert_document(
                folder_id=folder_id,
                raw_text=doc.page_content,
                chunk_sequence=doc.metadata.get("chunk_id", f"chunk_{i}"),
                text_embedding=embedding,
                metadata=doc.metadata
            )
            document_ids.append(document_id)
        
        logger.info(f"âœ… {len(documents)}ê°œ ë¬¸ì„œì™€ ì„ë² ë”©ì„ MongoDBì— ì €ì¥ì™„ë£Œ")
        return document_ids
    
    def similarity_search(self, query_embedding: List[float], 
                         folder_id: Optional[ObjectId] = None, 
                         k: int = 5) -> List[Document]:
        """ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰"""
        # MongoDB ë²¡í„° ê²€ìƒ‰ ì‚¬ìš©
        results = self.db_client.vector_search(query_embedding, folder_id, k)
        
        # Document ê°ì²´ë¡œ ë³€í™˜
        documents = []
        for result in results:
            doc_data = result["document"]
            document = Document(
                page_content=doc_data.get("raw_text", ""),
                metadata={
                    **doc_data.get("metadata", {}),
                    "document_id": str(doc_data["_id"]),
                    "folder_id": str(doc_data["folder_id"]),
                    "similarity": result["similarity"],
                    "chunk_sequence": doc_data.get("chunk_sequence", "")
                }
            )
            documents.append(document)
        
        return documents
    
    def similarity_search_with_score(self, query_embedding: List[float], 
                                   folder_id: Optional[ObjectId] = None, 
                                   k: int = 5) -> List[tuple]:
        """ì ìˆ˜ì™€ í•¨ê»˜ ìœ ì‚¬ë„ ê²€ìƒ‰"""
        results = self.db_client.vector_search(query_embedding, folder_id, k)
        
        scored_documents = []
        for result in results:
            doc_data = result["document"]
            document = Document(
                page_content=doc_data.get("raw_text", ""),
                metadata={
                    **doc_data.get("metadata", {}),
                    "document_id": str(doc_data["_id"]),
                    "folder_id": str(doc_data["folder_id"]),
                    "chunk_sequence": doc_data.get("chunk_sequence", "")
                }
            )
            scored_documents.append((document, result["similarity"]))
        
        return scored_documents
    
    def update_embeddings_batch(self, document_embeddings: Dict[ObjectId, List[float]]):
        """ë°°ì¹˜ë¡œ ì„ë² ë”© ì—…ë°ì´íŠ¸"""
        updated_count = 0
        
        for document_id, embedding in document_embeddings.items():
            try:
                self.db_client.update_document_embedding(document_id, embedding)
                updated_count += 1
            except Exception as e:
                logger.error(f"ì„ë² ë”© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ {document_id}: {str(e)}")
        
        logger.info(f"âœ… {updated_count}ê°œ ë¬¸ì„œ ì„ë² ë”© ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        return updated_count
    
    def migrate_from_faiss(self, faiss_vectorstore, embedder, folder_id: ObjectId):
        """FAISSì—ì„œ MongoDBë¡œ ë²¡í„° ë°ì´í„° ì´ê´€"""
        logger.info("ğŸ”„ FAISS â†’ MongoDB ë²¡í„° ì´ê´€ ì‹œì‘...")
        
        try:
            # FAISS ì¸ë±ìŠ¤ì—ì„œ ëª¨ë“  ë²¡í„°ì™€ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
            # ì£¼ì˜: ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” FAISS ì €ì¥ ë°©ì‹ì— ë”°ë¼ ì¡°ì • í•„ìš”
            if hasattr(faiss_vectorstore, 'docstore') and hasattr(faiss_vectorstore, 'index_to_docstore_id'):
                migrated_count = 0
                
                # FAISS ì €ì¥ëœ ë¬¸ì„œë“¤ ìˆœíšŒ
                for idx in range(faiss_vectorstore.index.ntotal):
                    try:
                        # ë¬¸ì„œ ID ê°€ì ¸ì˜¤ê¸°
                        doc_id = faiss_vectorstore.index_to_docstore_id.get(idx)
                        if doc_id is None:
                            continue
                        
                        # ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
                        document = faiss_vectorstore.docstore.search(doc_id)
                        if document is None:
                            continue
                        
                        # ë²¡í„° ê°€ì ¸ì˜¤ê¸°
                        vector = faiss_vectorstore.index.reconstruct(idx)
                        embedding = vector.tolist()
                        
                        # MongoDBì— ì €ì¥
                        document_id = self.db_client.insert_document(
                            folder_id=folder_id,
                            raw_text=document.page_content,
                            chunk_sequence=document.metadata.get("chunk_id", f"migrated_{idx}"),
                            text_embedding=embedding,
                            metadata={
                                **document.metadata,
                                "migrated_from": "faiss",
                                "original_index": idx
                            }
                        )
                        
                        migrated_count += 1
                        
                        if migrated_count % 100 == 0:
                            logger.info(f"ì§„í–‰ ìƒí™©: {migrated_count}ê°œ ë²¡í„° ì´ê´€ ì™„ë£Œ")
                            
                    except Exception as e:
                        logger.error(f"ì¸ë±ìŠ¤ {idx} ì´ê´€ ì‹¤íŒ¨: {str(e)}")
                        continue
                
                logger.info(f"âœ… FAISS â†’ MongoDB ì´ê´€ ì™„ë£Œ: {migrated_count}ê°œ ë²¡í„°")
                return migrated_count
                
        except Exception as e:
            logger.error(f"âŒ FAISS ì´ê´€ ì‹¤íŒ¨: {str(e)}")
            return 0
    
    def migrate_from_chroma(self, chroma_vectorstore, folder_id: ObjectId):
        """Chromaì—ì„œ MongoDBë¡œ ë²¡í„° ë°ì´í„° ì´ê´€"""
        logger.info("ğŸ”„ Chroma â†’ MongoDB ë²¡í„° ì´ê´€ ì‹œì‘...")
        
        try:
            # Chromaì—ì„œ ëª¨ë“  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            if hasattr(chroma_vectorstore, '_collection'):
                # Chroma ì»¬ë ‰ì…˜ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                collection = chroma_vectorstore._collection
                results = collection.get(include=['documents', 'embeddings', 'metadatas'])
                
                migrated_count = 0
                documents = results.get('documents', [])
                embeddings = results.get('embeddings', [])
                metadatas = results.get('metadatas', [])
                
                for i, (doc, embedding, metadata) in enumerate(zip(documents, embeddings, metadatas)):
                    try:
                        document_id = self.db_client.insert_document(
                            folder_id=folder_id,
                            raw_text=doc,
                            chunk_sequence=metadata.get("chunk_id", f"chroma_migrated_{i}"),
                            text_embedding=embedding,
                            metadata={
                                **(metadata or {}),
                                "migrated_from": "chroma",
                                "original_index": i
                            }
                        )
                        
                        migrated_count += 1
                        
                        if migrated_count % 100 == 0:
                            logger.info(f"ì§„í–‰ ìƒí™©: {migrated_count}ê°œ ë²¡í„° ì´ê´€ ì™„ë£Œ")
                            
                    except Exception as e:
                        logger.error(f"ì¸ë±ìŠ¤ {i} ì´ê´€ ì‹¤íŒ¨: {str(e)}")
                        continue
                
                logger.info(f"âœ… Chroma â†’ MongoDB ì´ê´€ ì™„ë£Œ: {migrated_count}ê°œ ë²¡í„°")
                return migrated_count
                
        except Exception as e:
            logger.error(f"âŒ Chroma ì´ê´€ ì‹¤íŒ¨: {str(e)}")
            return 0
    
    def search_by_metadata(self, metadata_filter: Dict[str, Any], 
                          folder_id: Optional[ObjectId] = None) -> List[Document]:
        """ë©”íƒ€ë°ì´í„°ë¡œ ê²€ìƒ‰"""
        query = {}
        
        if folder_id:
            query["folder_id"] = folder_id
        
        # ë©”íƒ€ë°ì´í„° í•„í„° ì¶”ê°€
        for key, value in metadata_filter.items():
            query[f"metadata.{key}"] = value
        
        results = list(self.collection.find(query))
        
        documents = []
        for doc_data in results:
            document = Document(
                page_content=doc_data.get("raw_text", ""),
                metadata={
                    **doc_data.get("metadata", {}),
                    "document_id": str(doc_data["_id"]),
                    "folder_id": str(doc_data["folder_id"]),
                    "chunk_sequence": doc_data.get("chunk_sequence", "")
                }
            )
            documents.append(document)
        
        return documents
    
    def get_document_count(self, folder_id: Optional[ObjectId] = None) -> int:
        """ë¬¸ì„œ ê°œìˆ˜ ì¡°íšŒ"""
        query = {}
        if folder_id:
            query["folder_id"] = folder_id
        
        return self.collection.count_documents(query)
    
    def delete_documents(self, document_ids: List[ObjectId]) -> int:
        """ë¬¸ì„œ ì‚­ì œ"""
        result = self.collection.delete_many({"_id": {"$in": document_ids}})
        logger.info(f"âœ… {result.deleted_count}ê°œ ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ")
        return result.deleted_count

class HybridVectorStore:
    """í•˜ì´ë¸Œë¦¬ë“œ ë²¡í„° ìŠ¤í† ì–´ (MongoDB + ê¸°ì¡´ ë²¡í„° DB)"""
    
    def __init__(self, mongo_vectorstore: MongoVectorStore, 
                 fallback_vectorstore=None):
        self.mongo_store = mongo_vectorstore
        self.fallback_store = fallback_vectorstore
    
    def similarity_search(self, query: str, embedder, k: int = 5, 
                         folder_id: Optional[ObjectId] = None) -> List[Document]:
        """í•˜ì´ë¸Œë¦¬ë“œ ìœ ì‚¬ë„ ê²€ìƒ‰"""
        # 1. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = embedder.embed_query(query)
        
        # 2. MongoDBì—ì„œ ê²€ìƒ‰
        mongo_results = self.mongo_store.similarity_search(query_embedding, folder_id, k)
        
        # 3. ì¶©ë¶„í•œ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ fallback ì‚¬ìš©
        if len(mongo_results) < k and self.fallback_store:
            fallback_results = self.fallback_store.similarity_search(query, k - len(mongo_results))
            mongo_results.extend(fallback_results)
        
        return mongo_results[:k]
    
    def add_documents(self, documents: List[Document], embeddings: List[List[float]], 
                     folder_id: ObjectId) -> List[ObjectId]:
        """ë¬¸ì„œ ì¶”ê°€ (MongoDBì—ë§Œ ì €ì¥)"""
        return self.mongo_store.add_documents(documents, embeddings, folder_id) 